{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMApCBtKa3/k8JwCHRZQ29O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/phongvu009/micro_grad/blob/main/Simple_Training_Loop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import math\n"
      ],
      "metadata": {
        "id": "5e9fME00ltOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(8)"
      ],
      "metadata": {
        "id": "HeJagT1tl5Ir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Value :\n",
        "  def __init__(self, data, _children=(), _op='', label = ''):\n",
        "    self.data = data\n",
        "    self._prev = _children\n",
        "    self._op = _op\n",
        "    self.label = label\n",
        "    self._backward = lambda : None\n",
        "    self.grad = 0.0\n",
        "\n",
        "  def __repr__(self):\n",
        "    return f\"Value(data={self.data})\"\n",
        "\n",
        "  def __add__(self,other):\n",
        "    #deal with integer : wrap integer into Value class\n",
        "    other = other if isinstance(other,Value) else Value(other)\n",
        "    out = Value(self.data + other.data, (self,other),  '+')\n",
        "    def _backward():\n",
        "      self.grad += 1.0 * out.grad\n",
        "      other.grad += 1.0 * out.grad\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "\n",
        "  def __neg__(self): # -self\n",
        "    return self * -1\n",
        "\n",
        "  def __sub__(self,other): # self - other\n",
        "    return self + (-other)\n",
        "\n",
        "  def __rsub__(self,other): #other - self\n",
        "    return other + (-self)\n",
        "\n",
        "  def __radd__(self,other):\n",
        "    return self + other\n",
        "  #deal with integer * Value_object\n",
        "  def __rmul__(self,other):\n",
        "    return self * other\n",
        "\n",
        "  def __mul__(self,other):\n",
        "    other = other if isinstance(other,Value) else Value(other)\n",
        "    out = Value(self.data * other.data, (self,other), '*')\n",
        "    def _backward():\n",
        "      self.grad += other.data * out.grad\n",
        "      other.grad += self.data * out.grad\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "\n",
        "  #do division\n",
        "  def __truediv__(self,other): #self/other\n",
        "    return self * other**-1\n",
        "\n",
        "  def __pow__(self,other):\n",
        "    assert isinstance(other,(int,float)) # only accept int or float\n",
        "    out = Value( self.data ** other, (self,) , f'**{other}')\n",
        "    def _backward():\n",
        "      self.grad += (other * self.data ** (other -1)) * out.grad\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "\n",
        "  def tanh(self):\n",
        "    x = self.data\n",
        "    t = (math.exp(2*x)-1) / (math.exp(2*x)+1)\n",
        "    out = Value(t, (self,), 'tanh')\n",
        "    def _backward():\n",
        "      self.grad += (1 - t**2) * out.grad\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "\n",
        "  def exp(self):\n",
        "    x = self.data\n",
        "    out = Value( math.exp(x), (self,), 'exp')\n",
        "    def _backward():\n",
        "      self.grad += out.data * out.grad\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "\n",
        "  def backward(self):\n",
        "    topo = []\n",
        "    visited = set()\n",
        "    def build_topo(current_node):\n",
        "      if current_node not in visited:\n",
        "        visited.add(current_node)\n",
        "        for child in current_node._prev:\n",
        "          build_topo(child)\n",
        "        topo.append(current_node)\n",
        "    build_topo(self)\n",
        "\n",
        "    self.grad = 1.0\n",
        "    for node in reversed(topo):\n",
        "      node._backward()\n",
        "\n",
        "class Neuron:\n",
        "  '''\n",
        "  To create a basic form: inputs -> 1 output\n",
        "  '''\n",
        "  def __init__(self,nin):\n",
        "    #initialize random weights and bias w.r.t inputs\n",
        "    self.w = [ Value(random.uniform(-1,1)) for _ in range(nin)]\n",
        "    self.b = Value(random.uniform(-1,1))\n",
        "\n",
        "  def __call__(self,x):\n",
        "    #zip() help to pair random weights with inputs\n",
        "    act = sum([wi*xi for wi,xi in zip(self.w,x) ])\n",
        "    out = act.tanh()\n",
        "    return out\n",
        "\n",
        "  def parameters(self):\n",
        "    return self.w + [self.b]\n",
        "\n",
        "class Layer:\n",
        "  def __init__(self,nin,nout):\n",
        "    self.neurons = [ Neuron(nin) for _ in range(nout) ]\n",
        "\n",
        "  def __call__(self,x):\n",
        "    outs = [n(x) for n in self.neurons]\n",
        "    return outs[0] if len(outs) == 1 else outs\n",
        "\n",
        "  def parameters(self):\n",
        "    return [ p for neuron in self.neurons for p in neuron.parameters()]\n",
        "\n",
        "class MLP:\n",
        "  def __init__(self, nin, nouts):\n",
        "    sz = [nin] + nouts\n",
        "    #to initilize weights and bias for more layer\n",
        "    self.layers = [ Layer(sz[i],sz[i+1]) for i in range(len(nouts)) ]\n",
        "\n",
        "  def __call__(self,x):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "    return x\n",
        "\n",
        "  def parameters(self):\n",
        "    #help to get param for multi-layer\n",
        "    return [ p for layer in self.layers for p in layer.parameters()]\n",
        "\n"
      ],
      "metadata": {
        "id": "lfBgeL7ll6kt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xs = [\n",
        "    [2.0, 3.0, -1.0],\n",
        "    [3.0, -1.0, 0.5],\n",
        "    [0.5, 1.0, 1.0],\n",
        "    [1.0, 1.0, -1.0],\n",
        "]\n",
        "\n",
        "ys =[1.0,-1.0,-1.0,1.0]"
      ],
      "metadata": {
        "id": "cKewB1n102kF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple Training Loop"
      ],
      "metadata": {
        "id": "zFDxSlVW1CrH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = MLP(3,[4,4,1])"
      ],
      "metadata": {
        "id": "FeBvT8W71NqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k in range(20):\n",
        "  #forward\n",
        "  pred = [n(x) for x in xs]\n",
        "  loss = sum( [(ys-pred)**2 for ys,pred in zip(ys,pred)])\n",
        "\n",
        "  #need to zero grad after backward : new grad after new update\n",
        "  # otherwise it will acc prevous grad\n",
        "  for p in n.parameters():\n",
        "    p.grad =0\n",
        "\n",
        "  #backward\n",
        "  loss.backward()\n",
        "\n",
        "  #update weights\n",
        "  for p in n.parameters():\n",
        "    p.data += -0.05 *  p.grad\n",
        "\n",
        "  print(f' at {k} -- the loss is : {loss.data} ')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKmoT5x60_WE",
        "outputId": "4345c8d7-a9b7-47f6-93d1-d83110c3f2d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " at 0 -- the loss is : 6.838684685676855 \n",
            " at 1 -- the loss is : 1.1781828422544471 \n",
            " at 2 -- the loss is : 0.5752382700337307 \n",
            " at 3 -- the loss is : 0.35630805750868577 \n",
            " at 4 -- the loss is : 0.24677779967573352 \n",
            " at 5 -- the loss is : 0.184369459913283 \n",
            " at 6 -- the loss is : 0.14519276471431294 \n",
            " at 7 -- the loss is : 0.11874483321217436 \n",
            " at 8 -- the loss is : 0.09988054007997223 \n",
            " at 9 -- the loss is : 0.0858421421938274 \n",
            " at 10 -- the loss is : 0.07503947057738908 \n",
            " at 11 -- the loss is : 0.06649958568193502 \n",
            " at 12 -- the loss is : 0.059597562477572044 \n",
            " at 13 -- the loss is : 0.05391550020742113 \n",
            " at 14 -- the loss is : 0.04916431555004119 \n",
            " at 15 -- the loss is : 0.04513819497728726 \n",
            " at 16 -- the loss is : 0.04168693924696691 \n",
            " at 17 -- the loss is : 0.038698557207396414 \n",
            " at 18 -- the loss is : 0.03608796257580577 \n",
            " at 19 -- the loss is : 0.03378942987645646 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HjaXNgV5q-x",
        "outputId": "ff177241-170d-4647-85fe-fcd60de5ff5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Value(data=0.9430602545179209),\n",
              " Value(data=-0.9260205026444556),\n",
              " Value(data=-0.8637048083926964),\n",
              " Value(data=0.919390137224767)]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EPIpsPjx5ur5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}